debug: false
run_name: '241208_train_frag_batchsize'
trainer:
  max_epochs: 500
  log_every_n_steps: 1000  # make this bigger if you don't want to log that often
  check_val_every_n_epoch: 1
checkpointer:
  monitor: 'val/pearsonr'
  mode: 'max'
data:
  dataset: 'data/super/241204_super_random_purified_data.csv'
  train_batch_size: 4
  val_batch_size: 32
  num_workers: 1
  validation_mode: label # random, top-k, label, or null
  val_label: val
  val_size: 0.1   # only used in random split 
  seed: 123     # seed for the dataloader..
  dataset_cfg:
    input_type: 'fragment' # embedding, sequence, or fragment
    label_col: 'z_score_norm_rate'
    name_col: 'name'
    seq_col: 'sequence'
    input_shape: 'BD' # BLD (keep seq length dim) or BD (flatten seq length dim)
    split_fragments_on: ':'    # split fragments on this character 
    path_connecting_variable: '-'  # connect fragments with this character
    num_fragments_in_subfolder: 2
    num_fragments: 4
    fragment_csv: 'data/super/super_fragment_data.csv'
    path_to_embeddings: '/projects/ml/itopt/datasets/esm_embeddings'
    embedding_key: null # use:  null for ESM   |   h_V for mpnn
model:
  lr: 1e-4    # learning rate
  num_models: 10    # number of models to train in parallel
  split_batch_mode: false # if true, batch size for each model will be B//N
  base_model:
    model_type: 'mlp'  # conv1d, mlp, each of the model of the ensemble 
    input_dim: 120
    hidden_dim: 16
    output_dim: 1     # always predicting one mean and one variance value
    kernel_size: 5   # kernel size for the conv1d model, size of the window that you scan with for the convld layer
    p_drop: 0.2 # dropout probability
    predict_variance: true # if true, predict variance
    variance_transform: "softplus" # softplus, sigmoid, clamp .. variance needs to be positive so this ensure it's always positive
  loss:
    loss_fn: 'nll' # mle, nll # loss function to use, mean squared error or negative log likelihood. 